{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Bhavya\n",
        "\n",
        "Week 09 Assignment - Scikit Learn"
      ],
      "metadata": {
        "id": "_d1Ai554pMgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among the different classification models included in the Python notebook, which model had the best overall performance? Support your response by referencing appropriate evidence.\n",
        "\n",
        "\n",
        "The Random Forest model performed as the top classification model according to results displayed in the Python notebook. The best overall performance emerges from the comparison between training accuracy scores and testing accuracy scores of all tested models. The Random Forest model outperformed logistic regression models which used L1 (LASSO) regularization and pipelines with scaling methods because it obtained superior accuracy metrics.\n",
        "\n",
        "The training accuracy of RandomForest_noCV reached almost perfect levels at 0.9993. The model displayed excessive overfitting because its high training accuracy of 0.9993 did not translate to better testing results which reached only 0.686. The model demonstrates memorization of training data instead of proper generalization yet it surpassed several logistic regression models in testing accuracy.\n",
        "\n",
        "The generalization performance of the Random Forest model received additional improvement through GridSearchCV implementation for cross-validation. The optimization process of hyperparameters including number of estimators and maximum number of features became possible through this technique. The testing accuracy of Random Forest increased after implementing optimization through cross-validation. The model performance improved because of implementing two grid searches to optimize tree depth.\n",
        "\n",
        "The training and testing accuracies of the logistic regression models with penalties L1 and L2 along with different C parameters stabilized at 0.73 and 0.71. These models demonstrated a stable predictive behavior along with reduced overfitting risk but delivered inferior accuracy results than Random Forest.\n",
        "\n",
        "The Random Forest model surpassed all logistic regression models in terms of accuracy performance notwithstanding the overfitting risks. The Random Forest method achieved peak accuracy results on training and testing data after performing cross-validation for hyperparameter optimization. The Random Forest model shows high predictive capabilities because it orchestrates multiple decision trees into a single effective framework."
      ],
      "metadata": {
        "id": "Gzyyt_-1pQiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, fit a series of logistic regression models, without regularization. Each model should use the same set of predictors (all of the relevant predictors in the dataset) and should use the entire dataset, rather than a fraction of it. Use a randomly chosen 80% proportion of observations for training and the remaining for checking the generalizable performance (i.e., performance on the holdout subset). Be sure to ensure that the training and holdout subsets are identical across all models. Each model should choose a different solver."
      ],
      "metadata": {
        "id": "eyOEGNcGpiJb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JV9vXANcoca-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from patsy import dmatrices\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import linear_model\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df_patient = pd.read_csv('./PatientAnalyticFile.csv')"
      ],
      "metadata": {
        "id": "s77MD29Fpl_r"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mortality variable\n",
        "df_patient['mortality'] = np.where(df_patient['DateOfDeath'].isnull(), 0, 1)\n",
        "\n",
        "# Convert DateOfBirth to date and calculate age\n",
        "df_patient['DateOfBirth'] = pd.to_datetime(df_patient['DateOfBirth'])\n",
        "df_patient['Age_years'] = ((pd.to_datetime('2015-01-01') - df_patient['DateOfBirth']).dt.days/365.25)\n",
        "\n",
        "# Create formula for all variables in model\n",
        "vars_remove = ['PatientID','First_Appointment_Date','DateOfBirth',\n",
        "              'Last_Appointment_Date','DateOfDeath','mortality']\n",
        "vars_left = set(df_patient.columns) - set(vars_remove)\n",
        "formula = \"mortality ~ \" + \" + \".join(vars_left)"
      ],
      "metadata": {
        "id": "FqMAXzCcpqox"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y, X = dmatrices(formula, df_patient)\n",
        "\n",
        "# Split Data into training and testing samples - using 80% for training\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                  np.ravel(Y),\n",
        "                                                  test_size=0.2,\n",
        "                                                  random_state=42)\n",
        "\n",
        "# Dictionary to store results\n",
        "results = []\n",
        "\n",
        "# List of solvers to try\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']"
      ],
      "metadata": {
        "id": "XlIHJtHqpsIi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for solver in solvers:\n",
        "    # Define model with specific solver\n",
        "    if solver == 'liblinear':\n",
        "        # liblinear doesn't support penalty=None\n",
        "        clf = linear_model.LogisticRegression(fit_intercept=True,\n",
        "                                              penalty='l2',\n",
        "                                              solver=solver,\n",
        "                                              C=1e10,\n",
        "                                              max_iter=1000)\n",
        "    else:\n",
        "        clf = linear_model.LogisticRegression(fit_intercept=True,\n",
        "                                              penalty=None,\n",
        "                                              solver=solver,\n",
        "                                              max_iter=1000)\n",
        "\n",
        "    # Time the fitting process\n",
        "    start_time = time.time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    fit_time = time.time() - start_time\n",
        "\n",
        "    # Calculate training and test accuracy\n",
        "    train_accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
        "    test_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
        "\n",
        "    # Store results\n",
        "    results.append({\n",
        "        'Solver': solver,\n",
        "        'Training Accuracy': train_accuracy,\n",
        "        'Holdout Accuracy': test_accuracy,\n",
        "        'Time (seconds)': fit_time\n",
        "    })\n",
        "\n",
        "# Create and display results table\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df[['Solver', 'Training Accuracy', 'Holdout Accuracy', 'Time (seconds)']]\n",
        "results_df['Training Accuracy'] = results_df['Training Accuracy'].round(4)\n",
        "results_df['Holdout Accuracy'] = results_df['Holdout Accuracy'].round(4)\n",
        "results_df['Time (seconds)'] = results_df['Time (seconds)'].round(2)\n",
        "\n",
        "print(results_df.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qok3h8FpwvP",
        "outputId": "e9e88670-4e38-47f7-f976-035dcbdc252d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Solver  Training Accuracy  Holdout Accuracy  Time (seconds)\n",
            "newton-cg             0.7481            0.7355            0.09\n",
            "    lbfgs             0.7480            0.7358            0.25\n",
            "liblinear             0.7479            0.7362            0.07\n",
            "      sag             0.7479            0.7358            2.61\n",
            "     saga             0.7480            0.7360            4.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results, which solver yielded the best results? Explain the basis for ranking the models - did you use training subset accuracy? Holdout subset accuracy? Time of execution? All three? Some combination of the three?\n",
        "\n",
        "\n",
        "The 'liblinear' solver demonstrated the highest holdout accuracy at 0.7362 because it provides the most accurate prediction of generalization ability. The 'liblinear' solver delivered slightly superior results than its counterparts which reached between 0.7355 and 0.7360 accuracy in the holdout evaluation. The slight variations in accuracy (0.0007 or 0.07%) between different solvers become significant when applied to large medical populations for predicting patient mortality.\n",
        "\n",
        "The execution time for 'liblinear' solver reaches 0.07 seconds making it the fastest available option. The execution time of 'liblinear' stands at 0.07 seconds which outperforms 'sag' (2.61 seconds) and 'saga' (4.45 seconds) and surpasses 'newton-cg' (0.09 seconds) and 'lbfgs' (0.25 seconds).\n",
        "\n",
        "The 'liblinear' solver demonstrates the best performance as the fastest and most accurate solution for this dataset when applied to this task. The efficient performance of this approach benefits real-world applications which demand high-speed processing together with excellent speed performance. The training accuracy holds little importance for model ranking because the holdout accuracy better demonstrates how well the model performs on new data.\n",
        "The accuracy differences between solvers remain small for this particular problem while the solver selection strongly affects computational speed.\n",
        "\n"
      ],
      "metadata": {
        "id": "xewe-RSnqr6R"
      }
    }
  ]
}